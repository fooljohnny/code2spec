# LLM 增强耗时根因分析

> 针对「上百接口仓库导出需数小时」的问题，分析耗时构成与可优化方向。

## 一、单次 LLM 请求耗时构成

对每个端点/错误码的一次 `enhanceEndpoint` 或 `enhanceErrorCode` 调用，耗时大致由以下部分组成：

```
总耗时 = llmDelayMs + 网络往返 + LLM 推理 + 本地处理
```

| 环节 | 当前配置 | 典型耗时 | 说明 |
|------|----------|----------|------|
| **llmDelayMs** | 2000ms | 2 秒 | 每次请求前固定 `Thread.sleep(2000)` |
| **网络往返** | - | 0.5~3 秒 | 取决于 API 地域、代理、网络质量 |
| **LLM 推理** | - | **10~120 秒** | 主要变量，见下文 |
| **本地处理** | - | <100ms | buildPrompt、parseJson 等 |

**结论**：单次请求中，LLM 推理和 llmDelayMs 是主要耗时来源。

---

## 二、LLM 推理为何会到「几分钟」

### 2.1 Prompt 规模

当前端点增强的 prompt 包含：

- System prompt：~200 字
- 接口信息：URI、方法名、Javadoc、参数、返回类型
- **方法体片段**：最多 1200 字（默认，可配置）
- **完整调用链代码**：最多 6000 字（默认，可配置；`--llm-minimal` 时不传）

精简模式（`--llm-minimal`）：仅传方法体约 800 字，不传调用链，单次输入约 500~1500 tokens。

### 2.2 输出与模型行为

- 输出：4 个 JSON 字段，约 100~300 tokens
- 模型：gpt-4o-mini 等，输入越长、输出越慢
- 若使用本地/较慢模型，单次 1~2 分钟很常见

### 2.3 可能出现的异常情况

- **读超时**：当前 60 秒，复杂 prompt 可能超时
- **429 限流**：每次重试前等待 60 秒，会显著拉长总时间

---

## 三、100 个端点的理论耗时估算

假设：

- 100 个端点 + 20 个错误码 = 120 次 LLM 调用
- 单次：2s 延迟 + 30s 推理 + 1s 网络 ≈ 33 秒

**串行总耗时**：120 × 33 秒 ≈ **66 分钟**

若单次推理到 60 秒：120 × 63 秒 ≈ **126 分钟（约 2 小时）**

---

## 四、可优化方向（供讨论）

### 4.1 减少 llmDelayMs（低风险）

- **现状**：每次请求前固定等待 2000ms
- **建议**：支持 `--llm-delay-ms 0` 或 500，由用户根据 API 限流策略选择
- **效果**：120 次 × 2s = 240 秒，可节省约 4 分钟；若设为 0，可节省约 4 分钟

### 4.2 压缩 Prompt 规模（中风险，需权衡质量）

| 手段 | 预期效果 | 风险 |
|------|----------|------|
| **`--llm-minimal`** | 仅传方法体 ~800 字，不传调用链，大幅减少 token | 无法分析调用链，文档质量下降 |
| 降低 `--llm-call-chain-max-chars`（默认已改为 6000） | 减少输入 token，加快推理 | 调用链信息变少，可能影响文档质量 |
| 降低 `--llm-call-chain-depth`（默认已改为 2） | 同上 | 同上 |
| 简化 system prompt | 略微减少 token | 对质量影响较小 |
| 对「简单接口」跳过 LLM | 减少调用次数 | 需定义「简单」规则，可能漏掉需要增强的接口 |

### 4.3 分批/批量调用（需 API 支持）

- 若所用 API 支持 batch：一次请求处理多个端点，可显著减少请求次数和总延迟
- 当前实现为单次 chat，需评估目标 API 是否提供 batch 接口

### 4.4 增量/缓存（中实现成本）

- 对「未变更」的接口复用上次 LLM 结果，只对变更部分重新调用
- 需要：文件 hash、结果缓存、失效策略

### 4.5 读超时与 429 策略

- **读超时**：对复杂 prompt 适当提高（如 120s），避免正常慢响应被误判超时
- **429**：可考虑指数退避、或提示用户调低并发/提高 delay，而不是简单固定 60 秒重试

### 4.6 选择性增强

- 仅对「重要」接口做 LLM 增强（如含 `@PostMapping`、`@PutMapping` 的写操作）
- 或通过配置/注解标记需要增强的接口

---

## 五、建议的优化优先级

| 优先级 | 方向 | 实现难度 | 预期收益 |
|--------|------|----------|----------|
| 1 | 将 llmDelayMs 设为可配置，支持 0 | 低 | 每次请求节省 0~2 秒 |
| 2 | 提高读超时（如 120s）并保持可配置 | 低 | 减少超时失败和重试 |
| 3 | 压缩 prompt（可配置 callChainMaxChars 等） | 低 | 缩短单次推理时间 |
| 4 | 选择性增强（仅部分接口调用 LLM） | 中 | 直接减少调用次数 |
| 5 | 增量/缓存 | 高 | 二次运行大幅加速 |

---

## 六、需要您补充的信息

为更准确判断瓶颈，建议补充：

1. **单次请求实际耗时**：从发起到收到完整响应的平均时间（可加简单日志）
2. **是否经常遇到 429**：若频繁，说明限流是主要因素
3. **使用的模型与 API**：不同模型/服务商的延迟差异很大
4. **典型 prompt 大小**：例如平均 token 数或字符数

有了这些数据，可以更有针对性地选择优化方案（例如优先减 delay、还是优先压 prompt、还是做选择性增强）。
